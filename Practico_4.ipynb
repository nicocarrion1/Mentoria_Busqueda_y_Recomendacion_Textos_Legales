{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05527142-55a9-4c45-b58b-caabcbacffd6",
   "metadata": {},
   "source": [
    "### Universidad Nacional de Córdoba - Facultad de Matemática, Astronomía, Física y Computación\n",
    "\n",
    "### Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones 2021\n",
    "Búsqueda y Recomendación para Textos Legales\n",
    "\n",
    "Mentor: Jorge E. Pérez Villella\n",
    "\n",
    "# Práctico Introducción al Aprendizaje Automático\n",
    "\n",
    "Integrantes: Carrion Nicolas, Delgado Gabriel\n",
    "    \n",
    "    \n",
    "\n",
    "El objetivo de este práctico es afianzar los conocimientos adquiridos hasta este momento, haciendo un proceso de re-análisis de los datos para encarar desde distintas perspectivas (selección de features, redefinición de clases y subclases) para conseguir nuevos resultados sobre los modelos ya trabajados, añadiendo ensamble learning al análisis.\n",
    "\n",
    "La idea es aprender a iterar en el proceso de ciencia de datos, no quedarnos con los resultados obtenidos del primer proceso realizado.\n",
    "\n",
    "Profundizar el tema de stop words y cómo generar uno propio.\n",
    "\n",
    "En este práctico, para resolver el problema de la clasificación se propone entrenar los siguientes modelos de la librería scikit-learn: LogisticRegretion y SGDClassifier.\n",
    "\n",
    "Fecha de Entrega: 12 de septiembre de 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dfb585b-416e-47a9-a2a7-319fdaabbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,roc_auc_score\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbb0b7b1-e3fd-4e31-8f7a-f6c226dd494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(dirName, quantity=None):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory\n",
    "    files = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    if not quantity:\n",
    "        for file in files:\n",
    "            # Create full path\n",
    "            fullPath = dirName + \"\\\\\" + file\n",
    "            # If entry is a directory then get the list of files in this directory \n",
    "            if os.path.isdir(fullPath) and not quantity:\n",
    "                allFiles = allFiles + getListOfFiles(fullPath)\n",
    "            else:\n",
    "                allFiles.append(fullPath)\n",
    "    else:\n",
    "        allFiles = allFiles + getListOfFiles(dirName)[:quantity]\n",
    "    return allFiles\n",
    "\n",
    "def create_corpus(file):\n",
    "    corpus=[]\n",
    "    f = open (file,'r', encoding=\"utf8\")\n",
    "    corpus=f.read()\n",
    "    return corpus\n",
    "\n",
    "\n",
    "cant_letters=2\n",
    "def normalize(s):\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "    )\n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b)\n",
    "    return s\n",
    "\n",
    "def limpieza_curacion(files):\n",
    "    lista=[]\n",
    "    for file in files:\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens=tokenizer.tokenize(file)\n",
    "\n",
    "        tokens_normalize = [normalize(word) for word in tokens]\n",
    "\n",
    "        tokens_normalize=[token for token in tokens_normalize if len(token) > cant_letters]\n",
    "\n",
    "        file_stopwords='stopwords.txt'\n",
    "        f = open (file_stopwords,'r', encoding=\"utf-8\")\n",
    "        stopwords_list=f.read()\n",
    "        #stopwords_list.replace('\\n', ' ')\n",
    "        stopwords_tokens=tokenizer.tokenize(stopwords_list)\n",
    "        stopwords_tokens=stopwords.words('spanish')\n",
    "        stopwords_tokens.extend(stopwords_tokens)\n",
    "        words = [token for token in tokens_normalize if token not in stopwords_tokens]\n",
    "\n",
    "\n",
    "        spanish_stemmer = SnowballStemmer('spanish')\n",
    "        tokens_stemm=[spanish_stemmer.stem(word) for word in words]\n",
    "\n",
    "        lista.append( ' '.join(tokens_stemm))\n",
    "    return lista\n",
    "\n",
    "\n",
    "def graph_frequency(dataframe):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=dataframe[:15].Token, y=dataframe[:15].Frecuencia, color='skyblue')\n",
    "    plt.xticks(rotation=90)\n",
    "    sns.despine()\n",
    "\n",
    "def obtain_tokens_and_dataframe(corpus):\n",
    "    tokens=[t for t in corpus.split()]\n",
    "    freq = nltk.FreqDist(tokens)\n",
    "    data = pd.DataFrame(freq.items(), columns=['Token', 'Frecuencia']).sort_values(by=\"Frecuencia\", ascending=False)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf2ad33-b1d2-4cb9-a67e-8242a3d77843",
   "metadata": {},
   "source": [
    "***CREACION DE STOPWORDS***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25897b4c-37cf-44ec-a9d5-e53f8405941e",
   "metadata": {},
   "source": [
    "Una parte muy importante de la limpieza de datos en NLP, es la eleccion de las stopwords.Estas, como mencionamos en el practico 2, son palabras que vamos a eliminar del texto que vamos a utilizar para entrenar. En el practico 2 se determino una lista de stopwords, basada en la lista de palabras que obtuvimos de un repositorio muy popular (https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt ) y de las stopwords propuestas por NLTK. Ahora ademas de esas listas, vamos a agregar una lista de stopwords creadas por nosotros, basandonos en diferentes tecnicas propuestas en el articulo (http://kavita-ganesan.com/tips-for-constructing-custom-stop-word-lists/#.YTedC470mUm)  Vamos a probar diferentes tecnicas con el corpus total y vamos a elegir solo una para volver a hacer el procesamiento de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a111d-2263-4887-9a3b-4acee0314b91",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "1. Términos más frecuentes como palabras vacías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc17112-6f6b-4ff7-8e30-325cc1de5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "fueros=['FAMILIA', 'LABORAL', 'MENORES', 'PENAL']\n",
    "root=os.getcwd()\n",
    "dirname=f'{root}\\\\Documentos'\n",
    "files=getListOfFiles(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9448a9-69b3-428c-985a-6011ce24a046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>de</td>\n",
       "      <td>82491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>la</td>\n",
       "      <td>58221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>que</td>\n",
       "      <td>40215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>el</td>\n",
       "      <td>35123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>en</td>\n",
       "      <td>33032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34699</th>\n",
       "      <td>abonadas-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34698</th>\n",
       "      <td>09);</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34697</th>\n",
       "      <td>(octubre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34696</th>\n",
       "      <td>(13.08.09)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63106</th>\n",
       "      <td>y551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63107 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Token  Frecuencia\n",
       "6              de       82491\n",
       "59             la       58221\n",
       "52            que       40215\n",
       "87             el       35123\n",
       "75             en       33032\n",
       "...           ...         ...\n",
       "34699   abonadas-           1\n",
       "34698        09);           1\n",
       "34697    (octubre           1\n",
       "34696  (13.08.09)           1\n",
       "63106        y551           1\n",
       "\n",
       "[63107 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "dirname= \"Corpus.txt\"\n",
    "\n",
    "f = open (dirname,'r', encoding=\"utf-8\")\n",
    "corpus=f.read()\n",
    "f.close\n",
    "corpus= corpus.lower()\n",
    "data=obtain_tokens_and_dataframe(corpus)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3db38e3b-047a-41f2-ab7d-887343e8b1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195411"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Frecuencia.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759aac5e-59f6-4ed3-9004-ea91796a6671",
   "metadata": {},
   "source": [
    "Vemos que la cantidad de palabras diferentes del corpus total es de 63107 y el total de tokens es 1195411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99ece6b-d859-4091-b69b-96194263f6b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>63107.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.942605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>531.024280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>82491.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Frecuencia\n",
       "count  63107.000000\n",
       "mean      18.942605\n",
       "std      531.024280\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        2.000000\n",
       "75%        4.000000\n",
       "max    82491.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a29a49a-c17e-4c13-88e5-69e157cb8e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.942605416197885"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Frecuencia.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb2163-8555-483f-b5df-0b18f4cefab1",
   "metadata": {},
   "source": [
    "La frecuencia media de todos las palabras es de 18,9, por lo cual podriamos eliminar las palabras que aparecen mas veces de lo que indica la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f6c0a3e-9169-426f-ab9d-f4dbb489d6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>de</td>\n",
       "      <td>82491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>la</td>\n",
       "      <td>58221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>que</td>\n",
       "      <td>40215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>el</td>\n",
       "      <td>35123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>en</td>\n",
       "      <td>33032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6001</th>\n",
       "      <td>turno.</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>-primer</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42772</th>\n",
       "      <td>coerción</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6006</th>\n",
       "      <td>papá</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>entendiendo</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5355 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Token  Frecuencia\n",
       "6               de       82491\n",
       "59              la       58221\n",
       "52             que       40215\n",
       "87              el       35123\n",
       "75              en       33032\n",
       "...            ...         ...\n",
       "6001        turno.          19\n",
       "4072       -primer          19\n",
       "42772     coerción          19\n",
       "6006          papá          19\n",
       "1597   entendiendo          19\n",
       "\n",
       "[5355 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_w_most_frecuency=data[data['Frecuencia']>data.Frecuencia.mean()]\n",
    "data_w_most_frecuency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f08c50-a98a-4091-a92b-05da46b17d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1036612"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_w_most_frecuency.Frecuencia.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9155f-fa73-4ded-993c-0d65ecb9415e",
   "metadata": {},
   "source": [
    "*La cantidad de palabras diferentes que eliminarias si dejamos esta lista como stopwords es de 5355 y de tokens totales de 1036612*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aeb123a-79e5-4687-bfef-94ae0c1e1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w_most_frecuency\n",
    "words_w_most_frecuency=list(data_w_most_frecuency.Token) #Asi generariamos la lista de stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a03a2-e0ee-4ed6-8e53-3c0568ee3438",
   "metadata": {
    "tags": []
   },
   "source": [
    "2. Términos menos frecuentes como palabras vacías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2ab34-0e7d-4856-89dd-fd0538bc023e",
   "metadata": {},
   "source": [
    "Si hacemos el proceso inverso y elegimos los terminos menos frecuentes como stopwords, hay que elegir un numero coherente, si hacemos el data.describe() de la frecuencia de los terminos podemos ver que el 50% de las palabas se repite al menos 2 veces. Por lo cual podria ser un buen indicador para tomar como limite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efcafd02-cca9-4241-9a8f-594a2375ecc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>63107.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.942605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>531.024280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>82491.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Frecuencia\n",
       "count  63107.000000\n",
       "mean      18.942605\n",
       "std      531.024280\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        2.000000\n",
       "75%        4.000000\n",
       "max    82491.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8cb461e-b299-4c04-a25b-762f86c576b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>advoctaus,</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44808</th>\n",
       "      <td>suceso-</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48658</th>\n",
       "      <td>presumirlo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37003</th>\n",
       "      <td>demonio</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31148</th>\n",
       "      <td>(tavip,</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34699</th>\n",
       "      <td>abonadas-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34698</th>\n",
       "      <td>09);</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34697</th>\n",
       "      <td>(octubre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34696</th>\n",
       "      <td>(13.08.09)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63106</th>\n",
       "      <td>y551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41038 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Token  Frecuencia\n",
       "3795   advoctaus,           2\n",
       "44808     suceso-           2\n",
       "48658  presumirlo           2\n",
       "37003     demonio           2\n",
       "31148     (tavip,           2\n",
       "...           ...         ...\n",
       "34699   abonadas-           1\n",
       "34698        09);           1\n",
       "34697    (octubre           1\n",
       "34696  (13.08.09)           1\n",
       "63106        y551           1\n",
       "\n",
       "[41038 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_w_lowest_frecuency=data[data['Frecuencia']<=2]\n",
    "data_w_lowest_frecuency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43e29aa7-aaba-4feb-b3d6-e8e01c458280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50889"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_w_lowest_frecuency.Frecuencia.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49709cda-68e1-4d8e-8b3c-6b3787a316fd",
   "metadata": {},
   "source": [
    "Aqui eliminariamos 41038 palabras diferentes y 50889 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e28190-8d36-4b46-b283-51f98bbe88a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56722</th>\n",
       "      <td>“smit”,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24327</th>\n",
       "      <td>forzado</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59638</th>\n",
       "      <td>(zabala).</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50209</th>\n",
       "      <td>festiva.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54103</th>\n",
       "      <td>nicomáquea</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Token  Frecuencia\n",
       "56722     “smit”,           1\n",
       "24327     forzado           2\n",
       "59638   (zabala).           1\n",
       "50209    festiva.           1\n",
       "54103  nicomáquea           1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_w_lowest_frecuency=list(data_w_lowest_frecuency.Token)\n",
    "data_w_lowest_frecuency.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2fc4d-0364-4e9f-8ca9-34c1eb5f7488",
   "metadata": {},
   "source": [
    "3. Términos de baja IDF como palabras vacías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cdb5c0-df60-4be5-9ace-1b3c6edb705d",
   "metadata": {},
   "source": [
    "La frecuencia inversa de documentos (IDF) básicamente se refiere a la fracción inversa de documentos en su colección que contiene un término específico ti. Digamos que tiene N documentos. Y el término X ocurrió en M de los N documentos. Por tanto, la IDF de X se calcula como:\n",
    "\n",
    "            IDF (X) = Log N / M\n",
    "Entonces, cuantos más documentos contengan X, menor será la puntuación de la IDF. Esto significa que los términos que aparecen en todos y cada uno de los documentos tendrán una puntuación IDF de 0. Si clasifica cada X en su colección por su puntuación IDF en orden descendente, puede tratar los K términos inferiores con las puntuaciones IDF más bajas como su parada palabras.\n",
    "\n",
    "Para poder hacer este calculo, no se puede utilizar un unicocorpus, si no que hay que pasarle al metodo un conjunto de documentos, cada uno con su corpus correspondiente. Esto es para poder hacer el calculo de aparicion de cada termino en cada documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ccf7cad-7bd4-4e01-83c2-e4067402fb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FAMILIA    124\n",
       "PENAL       53\n",
       "LABORAL     37\n",
       "MENORES     29\n",
       "Name: fuero, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.DataFrame(files, columns=['file'])\n",
    "data['fuero']= data['file'].apply(lambda x: x.split('\\\\')[-2])\n",
    "data['texto']= data['file'].apply(lambda x: create_corpus(x))\n",
    "#data['texto']= data['texto'].apply(lambda x: ' '.join(limpieza_curacion(x)))\n",
    "data['fuero'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b5f1670-bfb2-4d71-a7e7-9eb6ea6c1c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palabra</th>\n",
       "      <th>idf_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24094</th>\n",
       "      <td>alarcón</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5292</th>\n",
       "      <td>delimita</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12185</th>\n",
       "      <td>559</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>interpersonal</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26535</th>\n",
       "      <td>anfibológicos</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15951</th>\n",
       "      <td>avocada</td>\n",
       "      <td>5.804021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15952</th>\n",
       "      <td>801</td>\n",
       "      <td>5.804021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15953</th>\n",
       "      <td>reformula</td>\n",
       "      <td>5.804021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15957</th>\n",
       "      <td>liquidadas</td>\n",
       "      <td>5.804021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15031</th>\n",
       "      <td>denegados</td>\n",
       "      <td>5.804021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30063 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             palabra      idf_\n",
       "24094        alarcón  1.000000\n",
       "5292        delimita  1.000000\n",
       "12185            559  1.000000\n",
       "9883   interpersonal  1.000000\n",
       "26535  anfibológicos  1.000000\n",
       "...              ...       ...\n",
       "15951        avocada  5.804021\n",
       "15952            801  5.804021\n",
       "15953      reformula  5.804021\n",
       "15957     liquidadas  5.804021\n",
       "15031      denegados  5.804021\n",
       "\n",
       "[30063 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TfidfVectorizer(ngram_range=(1,1))\n",
    "tfidf = transformer.fit(data['texto'])\n",
    "df_tfidf = pd.DataFrame(transformer.idf_,columns=[\"idf_\"])\n",
    "df_tfidf.insert(0,'palabra',tfidf.vocabulary_.keys())\n",
    "df_tfidf.sort_values('idf_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dfcacc3-b031-4265-99d8-0bfd6f72b8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30063.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.979908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.051303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.551258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.398556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.804021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.804021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               idf_\n",
       "count  30063.000000\n",
       "mean       4.979908\n",
       "std        1.051303\n",
       "min        1.000000\n",
       "25%        4.551258\n",
       "50%        5.398556\n",
       "75%        5.804021\n",
       "max        5.804021"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e4bd6-6a8f-4dcd-8d7d-ab124644a960",
   "metadata": {},
   "source": [
    "Podemos eliminar el 20% de palabras con menos importancia segun el calculo del tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc63c762-169b-4616-802b-f07183163385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palabra</th>\n",
       "      <th>idf_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auto</td>\n",
       "      <td>3.000661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122</td>\n",
       "      <td>2.585145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>art</td>\n",
       "      <td>2.508184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>violencia</td>\n",
       "      <td>2.090449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>secretaría</td>\n",
       "      <td>2.078328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30055</th>\n",
       "      <td>contribuían</td>\n",
       "      <td>2.859582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30057</th>\n",
       "      <td>acogieron</td>\n",
       "      <td>2.042821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30058</th>\n",
       "      <td>paraguas</td>\n",
       "      <td>3.789118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30061</th>\n",
       "      <td>contemplase</td>\n",
       "      <td>3.361674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30062</th>\n",
       "      <td>y551</td>\n",
       "      <td>3.724580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5690 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           palabra      idf_\n",
       "0             auto  3.000661\n",
       "1              122  2.585145\n",
       "19             art  2.508184\n",
       "27       violencia  2.090449\n",
       "31      secretaría  2.078328\n",
       "...            ...       ...\n",
       "30055  contribuían  2.859582\n",
       "30057    acogieron  2.042821\n",
       "30058     paraguas  3.789118\n",
       "30061  contemplase  3.361674\n",
       "30062         y551  3.724580\n",
       "\n",
       "[5690 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_w_lower_idf=df_tfidf[df_tfidf['idf_'] < df_tfidf['idf_'].quantile(0.20)]\n",
    "data_w_lower_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb880a38-68f2-42a6-a382-abd2080bdacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0992729524948315"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_w_lower_idf.idf_.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0546f288-e8af-4053-ac1c-a22cd0ce311d",
   "metadata": {},
   "source": [
    "Si tomamos como stopword las palabras que tienen un idf menor a 4,09 eliminariamos 5690 palabras del actual corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "934465ca-d78f-4b5a-ad63-6474358adf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5690"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_w_lower_idf.palabra.values\n",
    "len(data_w_lower_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f961bfc8-7e0a-406c-9f38-4193157eef99",
   "metadata": {},
   "source": [
    "Vamos a utilizar esta ultima tecnica como metodo para seleccionar nuevos stopwords, ademas de los que ya teniamos en el practico 2. Asi que vamos a crear una nueva lista de stopwords que contenga los stopwords viejos y nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e6841c7-14fc-4857-b540-023b4087d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "file_stopwords='stopwords.txt'\n",
    "f = open (file_stopwords,'r', encoding=\"utf-8\")\n",
    "stopwords_list=f.read()\n",
    "stopwords_tokens_list=tokenizer.tokenize(stopwords_list)\n",
    "stopwords_tokens=stopwords.words('spanish')\n",
    "stopwords_tokens.extend(stopwords_tokens_list)\n",
    "stopwords_tokens.extend(data_w_lower_idf.palabra.values) #Aca se agregar los nuevos stopwords\n",
    "STOPWORDS =set([normalize(word.lower()) for word in stopwords_tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f5aeb-4fa8-4b9c-b812-60c959c80195",
   "metadata": {},
   "source": [
    "***Limpieza del corpus***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c39248a-6eec-4291-b026-c1102b3db4ca",
   "metadata": {},
   "source": [
    "Una vez generado la lista de stopwords extendida a lo que ya teniamos en el practico 2, vamos a generar la funcion de limpieza que tambien habiamos creado en el practico mencionado, repasando los procesamientos mas importantes que aplicabamos:\n",
    "\n",
    "\n",
    "- Tokenizamos el corpus\n",
    "- Lo normalizamos, quitando los acentos de las palabras\n",
    "- Eliminamos las palabras que tienen menos de 3 letras\n",
    "- Eliminamos los stopwords\n",
    "- Aplicamos stemming\n",
    "\n",
    "\n",
    "Ademas de los pasos mencionados, decidimos eliminar los numeros presentes en el corpus con la funcion isalpha().\n",
    "\n",
    "Una vez aplicado esto pudimos observar que el corpus quedaba lo suficientemente limpio, ya que en el practico 2 se aplicaron bastante tecnicas de preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e927662b-063a-429f-8674-824b127818d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_curacion(file):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens=tokenizer.tokenize(file)\n",
    "\n",
    "    tokens_normalize = [normalize(word) for word in tokens if word.isalpha()]\n",
    "\n",
    "    tokens_normalize=[token for token in tokens_normalize if len(token) > cant_letters]\n",
    "\n",
    "    words = [token for token in tokens_normalize if token not in STOPWORDS]\n",
    "\n",
    "\n",
    "    spanish_stemmer = SnowballStemmer('spanish')\n",
    "    tokens_stemm=[spanish_stemmer.stem(word) for word in words]\n",
    "    return tokens_stemm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bab83d4e-1df4-4735-a5b9-7b8f2624e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus= limpieza_curacion(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43bdd5fd-0bab-4180-b96b-78efe739aeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mil',\n",
       " 'particul',\n",
       " 'casacion',\n",
       " 'sig',\n",
       " 'premur',\n",
       " 'ccc',\n",
       " 'estudi',\n",
       " 'ocasion',\n",
       " 'cient',\n",
       " 'plaz']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(clean_corpus, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17406f31-ad08-4c58-8290-f406047a19f3",
   "metadata": {},
   "source": [
    "***Generacion de Pipeline y entrenamiento***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c754d59-aecd-428a-b0ba-159d9842c4b9",
   "metadata": {},
   "source": [
    "Para poder facilitar la realizacion del preprocesamiento de los textos, asi como su vectorizacion, entrenamiento y evaluacion se pueden generar pipelines de ejecucion con diferentes steps en donde el output de cada step, es el input del step siguiente. Es importante recalcar que para que diferentes steps funcionen dentro del Pipeline de sklearn que es el que vamos a estar usando, tienen que si o si implementar las funciones **fit** y **transform**\n",
    "\n",
    "El Pipeline de sklearn permite Aplicar secuencialmente una lista de transformaciones y un estimador final. Los pasos intermedios de la tubería deben ser 'transformaciones', es decir, deben implementar métodos **fit** y **transform**. El estimador final solo necesita implementar el ajuste. Los transformadores en la tubería se pueden almacenar en caché usando memoryargumento.\n",
    "\n",
    "El propósito de la canalización es ensamblar varios pasos que se pueden validar juntos mientras se establecen diferentes parámetros. Para ello, permite configurar los parámetros de los distintos pasos utilizando sus nombres y el nombre del parámetro separados por un '__', como en el ejemplo siguiente. El estimador de un paso se puede reemplazar por completo configurando el parámetro con su nombre en otro estimador, o se puede eliminar un transformador configurándolo en 'passthrough' o None.\n",
    "\n",
    "Si queremos mas detalles de la implementacion podemos consultar en el codigo fuente https://github.com/scikit-learn/scikit-learn/blob/2beed5584/sklearn/pipeline.py#L166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba1c5a52-e49e-4b95-845f-806519fdbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(data['texto'], data['fuero'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec87563-b731-411d-89fa-a6af8f6cc0f0",
   "metadata": {},
   "source": [
    "Teniendo en cuenta la informacion previa, vamos a crear una clase que a cada documento, le aplique la funcion de limpieza y curacion para un primer step del Pipeline, respetando las restricciones propuestas por sklearn de crear el metodo fit y transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fab686e0-d235-4fe9-aeda-76f3dd1ed6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "\n",
    "    def transform(self, x, y=None):\n",
    "        resultado=[]\n",
    "        for i in x:\n",
    "            resultado.append( ' '.join(limpieza_curacion(i)))\n",
    "        return resultado\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff4733-77d3-476b-9205-1a46e0e81bd4",
   "metadata": {},
   "source": [
    "***Regresion Logistica***\n",
    "\n",
    "La regresión logística es un método estadístico que trata de modelar la probabilidad de una variable cualitativa binaria (dos posibles valores) en función de una o más variables independientes. \n",
    "La principal aplicación de la regresión logística es la creación de modelos de clasificación binaria.\n",
    "\n",
    "Regresión logística multinomial es una extensión de la regresión logística que agrega soporte nativo para problemas de clasificación de clases múltiples.\n",
    "\n",
    "- **Regresión logística binomial:** Regresión logística estándar que predice una probabilidad binomial (es decir, para dos clases) para cada ejemplo de entrada.\n",
    "- **Regresión logística multinomial:** Versión modificada de la regresión logística que predice una probabilidad multinomial (es decir, más de dos clases) para cada ejemplo de entrada.\n",
    "\n",
    "***Optimizacion de hiperparametros con GridSearch***\n",
    "\n",
    "Para la optimizacion de hiperparametros con GridSearch utilizamos como metrica de scoring el F1, debido a que cuando las clases estan desbalanceadas, no se recomienda como buena practica utilizar el accuracy\n",
    "- penalty: Se utiliza para especificar la norma utilizada en la penalización. (Link con data interesante sobre los penalties l1 y l2 https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)\n",
    "- C: Inversa de la fuerza de regularización; debe ser un flotador positivo. Al igual que en las máquinas de vectores de soporte, los valores más pequeños especifican una regularización más fuerte.\n",
    "- max_iter: Número máximo de iteraciones que se toman para que los solucionadores converjan.\n",
    "- solver: Algoritmo a utilizar en el problema de optimización.Para conjuntos de datos pequeños, 'liblinear' es una buena opción, mientras que 'sag' y 'saga' son más rápidos para los grandes.Para problemas multiclase, solo 'newton-cg', 'sag', 'saga' y 'lbfgs' manejan la pérdida multinomial; 'liblinear' se limita a esquemas uno versus resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df688029-e824-4da3-995c-d26473d9798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     FAMILIA       1.00      1.00      1.00        37\n",
      "     LABORAL       1.00      1.00      1.00        11\n",
      "     MENORES       1.00      1.00      1.00         9\n",
      "       PENAL       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        73\n",
      "   macro avg       1.00      1.00      1.00        73\n",
      "weighted avg       1.00      1.00      1.00        73\n",
      "\n",
      "Mejores hiperparametros:  {'model__C': 0.1, 'model__max_iter': 5, 'model__penalty': 'none', 'model__solver': 'saga'}\n",
      "Wall time: 25min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = {'model__penalty': ['l1', 'l2', 'none'],\n",
    "              'model__solver': ['lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "              'model__C': [0.1, 1, 10],\n",
    "              'model__max_iter': [5]}\n",
    "\n",
    "clf = Pipeline([('normalizer', Normalizer()),('TfidfVectorizer', TfidfVectorizer()), ('model',LogisticRegression())])\n",
    "cv = GridSearchCV(clf, param_grid, scoring = 'f1_macro', refit=True, cv=5, n_jobs = -1)\n",
    "cv.fit(X_train,y_train)\n",
    "test_predictions=cv.predict(X_test)\n",
    "print(classification_report(test_predictions, y_test,zero_division=0))\n",
    "print('Mejores hiperparametros: ', cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6229bb-defb-440e-811a-4500ea18334f",
   "metadata": {},
   "source": [
    "*Con optimizacion de hiperparametros el modelo y a la perfeccion al conjunto de test, de acuerdo a lo que podemos ver en los resultado.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f5c0b-9a5a-4dc4-a653-d848c55a8924",
   "metadata": {},
   "source": [
    "***Descenso de gradiente estocástico(SGD)***\n",
    "\n",
    "El descenso de gradiente estocástico (SGD) es un enfoque simple pero muy eficiente para ajustar clasificadores y regresores lineales bajo funciones de pérdida convexa como máquinas de vectores de soporte (lineales) y regresión logística.\n",
    "El SGD se ha aplicado con éxito a los problemas de aprendizaje de las máquinas a gran escala y escasas que se encuentran a menudo en la clasificación de textos y el procesamiento del lenguaje natural.\n",
    "Estrictamente hablando, SGD es simplemente una técnica de optimización y no corresponde a una familia específica de modelos de aprendizaje automático. Es solo una forma de entrenar a un modelo.\n",
    "\n",
    "Por ejemplo, usar SGDClassifier(loss='log') da como resultado una regresión logística, es decir, un modelo equivalente a LogisticRegression que se ajusta a través de SGD en lugar de ser ajustado por uno de los otros solucionadores en LogisticRegression.\n",
    "\n",
    "***Optimizacion de hiperparametros con GridSearch***\n",
    "\n",
    "Para la optimizacion de hiperparametros con GridSearch utilizamos como metrica de scoring el F1, debido a que cuando las clases estan desbalanceadas, no se recomienda como buena practica utilizar el accuracy\n",
    "\n",
    "- loss: Se utiliza para especificar la funcion de perdida(”hinge”: Máquina Vectorial de Soporte lineal 'SVM', ”log”: regresión logística).\n",
    "- learning_rate: Nos dice que tanto actualizamos los pesos en cada iteración. ”constant”: eta = eta0, ”optimal”: eta = 1.0 / (alpha * (t + t0))d onde t0 es elegida por una heurística propuesta por Leon Bottou y por ultimo ”adaptive”: eta = eta0, siempre que el entrenamiento siga disminuyendo. Cada vez que n_iter_no_change épocas consecutivas no logran disminuir la pérdida de entrenamiento en tol o no aumentan la puntuación de validación en tol si early_stopping es True, la tasa de aprendizaje actual se divide por 5. \n",
    "- alpha: Constante que multiplica el plazo de regularización. Cuanto mayor sea el valor, más fuerte será la regularización. También se utiliza para calcular la tasa de aprendizaje cuando se establece en learning_rate como ”optimal”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4f5eb36-8be1-4554-9372-8b811e6a8062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     FAMILIA       1.00      0.66      0.80        56\n",
      "     LABORAL       0.91      1.00      0.95        10\n",
      "     MENORES       0.00      0.00      0.00         0\n",
      "       PENAL       0.44      1.00      0.61         7\n",
      "\n",
      "    accuracy                           0.74        73\n",
      "   macro avg       0.59      0.67      0.59        73\n",
      "weighted avg       0.93      0.74      0.80        73\n",
      "\n",
      "Mejores hiperparametros:  {'model__alpha': 0.1, 'model__learning_rate': 'optimal', 'model__loss': 'hinge'}\n",
      "Wall time: 24min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "param_dist = {\n",
    "    'model__loss': [\n",
    "        'hinge',        # SVM\n",
    "        'log',          # logistic regression\n",
    "    ],\n",
    "    'model__learning_rate':['constant', 'optimal','adaptive'],\n",
    "    'model__alpha': [0.1,0.5,1,5,10,50]}\n",
    "\n",
    "clf = Pipeline([('normalizer', Normalizer()),('TfidfVectorizer', TfidfVectorizer()), ('model',SGDClassifier(random_state=42, eta0=0.1))])\n",
    "cv = GridSearchCV(clf, param_dist, scoring = 'f1_macro', refit=True, cv=5, n_jobs=-1)\n",
    "cv.fit(X_train,y_train)\n",
    "test_predictions=cv.predict(X_test)\n",
    "print(classification_report(test_predictions, y_test,zero_division=0))\n",
    "print('Mejores hiperparametros: ', cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad01087e-1264-49c7-8963-1550fabbf9c1",
   "metadata": {},
   "source": [
    "*Como podemos observar las metricas no fueron las mejores, mas teniendo en cuenta que SVM y regresion logistica (vistas en el practico anterior), nos arrojaron mejores resultados.\n",
    "Ademas tanto learning_rate como loss, se considero que lo mas optimo sean los valores por defecto. Mientras que el alpha si se aleja bastente de su valor por defecto que es 0.0001*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4368db-a3c5-4599-8976-dc051dbfb3b9",
   "metadata": {},
   "source": [
    "***Conclusion***\n",
    "\n",
    "En este trabajo practico, de alguna forma hicimos una iteracion incremental con respecto a los temas que ya habiamos visto en los trabajos practicos anteriores, para mejorar el preprocesamiento de los datos. Algunos de los procesamientos que aplicamos fueron:\n",
    "- Eliminar los tokens que eran numeros, ya que no consideramos que sean importantes a la hora de clasificar\n",
    "- Se creo una lista de stopwords propia. A partir de diferentes alternativas contempladas, nos parecio la mas optima la generada a partir de coeficiente proporcionado por TDFIDF.\n",
    "- Se genero un Pipeline que aplica de manera secuencial, la limpieza de datos, la vectorizacion y la estimacion generada por un modelo dado.\n",
    "\n",
    "Posterior a esto, probamos entrenar y evaluar con Logistic Regression y SDGClassifier. \n",
    "Con Logistic Regression obtuvimos muy buenos resultados tal como habia sucedido en el practico anterior. En cambio con SGDClassifier, no obtuvimos buenas metricas, lo cual podria ser por el desbalanceo de los datos y la cantidad baja de ejemplos (En los fueros donde hay mas archivos, el modelo predice mejor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c25d6-6b23-4951-bc1d-fd949a588ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239c1af-efea-49ef-bb6f-ed16e8ade482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mentoria_env] *",
   "language": "python",
   "name": "conda-env-mentoria_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
